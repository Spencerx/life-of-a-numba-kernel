{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The life of a Numba kernel - from Python source to kernel launch\n",
    "\n",
    "**Author:** Graham Markall ([@gmarkall](https://twitter.com/gmarkall))\n",
    "\n",
    "This notebook gives an overview of the compiler and runtime pipeline that Numba uses to take Python source code, turn it into a CUDA kernel, and launch it. It pulls in a variety of Numba internals to illustrate how the different parts of the pipeline work.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Numba CUDA target (or CUDA Python, as it's sometimes referred to) has three components:\n",
    "\n",
    "1. A NumPy-like array library backed by CUDA\n",
    "2. A Python-to-PTX compiler that uses NVVM\n",
    "3. A Python interface to the CUDA driver API\n",
    "\n",
    "The first part of this notebook is about the Python-to-PTX compiler that uses NVVM, and this follows on to an overview of the Python interface to the CUDA driver API. We will follow through the process that starts with a Python function, and ends with the execution of a kernel on the GPU.\n",
    "\n",
    "## A simple kernel\n",
    "\n",
    "Throughout the notebook we'll use a simple L1 BLAS-like kernel for the example. Let's import Numba and NumPy, and define the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def axpy(r, a, x, y): \n",
    "    i = cuda.grid(1)\n",
    "\n",
    "    if i < len(r):\n",
    "        r[i] = a * x[i] + y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "a = 5.0 \n",
    "x = np.arange(N).astype(np.int32)\n",
    "y = np.random.randint(100, size=N).astype(np.int32)\n",
    "r = np.zeros_like(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and launch the kernel using that data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = 256 \n",
    "blocks = (N // threads) + 1 \n",
    "axpy[blocks, threads](r, a, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick sanity check to make sure it's done as we expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(np.all(r == a * x + y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What just happened?\n",
    "\n",
    "When we configured and launched the kernel, Numba jumped in and compiled the kernel, transferred data to the GPU, launched the kernel on the data, and transferred the result back. How did Numba compile and load the kernel to the GPU? The process is outlined in this flowchart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import SVG\n",
    "SVG(filename='numba-pipeline.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll follow through each of the steps in the notebook, mostly in code. Some of the following code uses various bits of Numba internals to get things done - this is to illustrate a little more clearly what Numba is doing, but be aware that in general it's not possible to drive Numba's pipeline from an external program.\n",
    "\n",
    "## Bytecode compilation\n",
    "\n",
    "Numba's input language is not Python source code / ASTs, but instead Python bytecode. The CPython interpreter compiles the source to Python bytecode - we can see what this looks like for our `axpy` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dis import dis\n",
    "dis(axpy.py_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CPython bytecode is a sequence of instructions for a simple stack machine. The line numbers corresponding to each sequence of instructions is given in the first column (3, 5, and 6). The next column is the offset of each instruction in the bytecode - e.g. at location 20 we have `POP_JUMP_IF_FALSE 46`, which branches to location 46. The next column is the opcode - all opcodes are listed in [this section of the Python dis module documentation](https://docs.python.org/3/library/dis.html#python-bytecode-instructions). The next column is an operand, whose meaning varies between the opcodes - for example the operand to `POP_JUMP_IF_FALSE` is the location to jump to, whereas the operand for the `COMPARE_OP` is the type of comparison to perform - in this case, `0` indicates `<` - note that many instructions also pop operands from the stack - for example, the `BINARY_SUBSCR` uses the three items loaded to the top of the stack by the `LOAD_FAST` instructions at locations 22, 24, and 26.\n",
    "\n",
    "Further reading: \n",
    "* [Numba manual: Bytecode Analysis](http://numba.pydata.org/numba-doc/dev/developer/architecture.html#stage-1-analyze-bytecode) - provides a slightly larger bytecode example.\n",
    "* [Stack Machines on Wikipedia](https://en.wikipedia.org/wiki/Stack_machine) - not the greatest introduction, but contains a lot of information.\n",
    "\n",
    "## Bytecode Analysis\n",
    "\n",
    "Numba's first step is to translate the Python Bytecode into Untyped Numba IR - this is a representation closer to the LLVM IR that Numba will eventually hand to NVVM. Its design provides\n",
    "\n",
    "* An abstract machine with infinite registers\n",
    "* A representation that type inference can be executed on\n",
    "* Support for rewriting the IR - transformations that make it easier to implement Python semantics in a lower-level representation.\n",
    "\n",
    "Numba does two forms of analysis on the Python bytecode, which it then uses to construct the Numba IR:\n",
    "\n",
    "* Control Flow Analysis\n",
    "* Data Flow Analysis\n",
    "\n",
    "To construct these, we use Numba's *Bytecode Interpreter*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba.core.bytecode import ByteCode, FunctionIdentity\n",
    "from numba.core.interpreter import Interpreter\n",
    "\n",
    "fi = FunctionIdentity.from_function(axpy.py_func)\n",
    "interp = Interpreter(fi)\n",
    "bc = ByteCode(fi)\n",
    "ir = interp.interpret(bc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first examine the Control Flow Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.cfa.dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CFA provides the control flow graph in terms of the basic blocks of the function - in this function there are three basic blocks, numbered 0, 24, and 48. The adjacency list gives the control flow between the basic blocks - the existence of an edge implies control can flow from the source block to the destination block.\n",
    "\n",
    "Knowing which nodes dominate which other nodes is also useful for various analyses - a node A dominates the node B if all control flow paths to B must pass through A.\n",
    "\n",
    "Post-dominators are a bit like the reverse of a dominator - a node C post-dominates a node D if all paths starting at D must pass through C to exit the graph.\n",
    "\n",
    "The text-based representation contains lots of information but is not that easy to interpret - Numba also provides a visual rendering of the CFG to help with interpreting it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = interp.cfa.graph.render_dot()\n",
    "dg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph in this case is rather simple - in block 0, we call `cuda.grid(1)` - if `i < len(r)`, control flow goes to block 24 then 48, otherwise it goes straight to 48 (the exit block).\n",
    "\n",
    "The data flow analysis can also be viewed, but there is not such a convenient way to view it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for block, abi in interp.dfa.infos.items():\n",
    "    print(f\"Block {block}:\\n\")\n",
    "    print(\"Instructions:\")\n",
    "    pprint(abi.insts)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Numba IR\n",
    "\n",
    "We can view the Numba IR too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir.dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the IR shows some correspondences between it and:\n",
    "\n",
    "* The Python bytecode - we can see some similar semantics, for example the loading of globals and methods, and the calling of methods\n",
    "* The names of the variables in the IR and the names in the Data Flow Analysis\n",
    "* The control flow analysis - the structure of control flow of the Numba IR matches that in the bytecode, and produced by the CFA.\n",
    "\n",
    "However, it is also a little different from the previous items:\n",
    "\n",
    "* The Numba IR abstract machine is not a stack machine, but instead has an infinite number of variables (`$2load_global0`, etc).\n",
    "* It doesn't explicitly hold the results of analysis, like the dominator and post-dominator analysis.\n",
    "\n",
    "We can also view the Numba IR as a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir_dg = ir.render_dot()\n",
    "ir_dg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewrite passes\n",
    "\n",
    "Some rewrites on the untyped IR occur at this point - e.g.\n",
    "\n",
    "* Some constant propagation, e.g. where one side of a binary operation has a constant value\n",
    "* Rewrite getitems where the index is known to be a constant\n",
    "* Rewrite raise expressions with constant arguments so they can be used in nopython mode.\n",
    "\n",
    "There is no easy way to look at these in the notebook, but most of them can be found in the `numba.core.rewrites` module, decorated with `@register_rewrite('before-inference')`.\n",
    "\n",
    "## Type inference\n",
    "\n",
    "Next is type inference. This starts with the types of the arguments and propagates type information through the function until everything has been typed. It uses:\n",
    "\n",
    "* Internally stored mappings of input types to output types - e.g. \"an add of a float32 and a float64 will have a return type of float64\"\n",
    "* The data flow graph\n",
    "\n",
    "When there are two incoming types for a variable, e.g. when two different values can be assigned to it because of branching, Numba constructs a set of possible types for the variable, and attempts to \"unify\" them - that is, choose a type that all of the types in the set can be safely coerced to.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "def select(a, b, threshold, value):\n",
    "    if threshold < value:\n",
    "        r = a   # r: float32\n",
    "    else:\n",
    "        r = b   # r: int32\n",
    "    return r    # r: {float32, int32} unifies to float64\n",
    "\n",
    "a = np.float32(1)\n",
    "b = np.int32(2)\n",
    "select(a, b, 10, 11) # Call with (float32, int32, int64, int64)\n",
    "```\n",
    "\n",
    "In the above example `r` is typed as `float32` in the taken branch, and `int32` in the not-taken branch. On the line with the return, the control flow from the if-else re-joins, so the types of `r` from both sides of it are used to construct the set of types. Since the set of types contains different types (`int32` and `float32`), Numba uses a built-in rule for unification - in this case, to ensure the best representation of values from all branches, it chooses `float64`.\n",
    "\n",
    "Further reading: [Numba Enhancement Proposal 1: Changes in Integer Typing](http://numba.pydata.org/numba-doc/latest/proposals/integer-typing.html) explains some of the rationale behind some of the decisions made by Numba's type system.\n",
    "\n",
    "### Inspecting the result of type inference\n",
    "\n",
    "We can use the `inspect_types()` function of a jitted kernel to see Numba's typing of all values in the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axpy.inspect_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only see one typing because we have called the function with one set of argument types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axpy.definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions are indexed by a tuple of `(compute capability, argument types)`. Let's call it with a different set, and see how this influences the set of definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "r = r.astype(np.float32)\n",
    "axpy[blocks, threads](r, a, x, y)\n",
    "\n",
    "axpy.definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see that there are two different definitions, which will have two different typings. Because the typings are different, the generated code for the definitions also differ from each other - we now have two different kernel objects.\n",
    "\n",
    "If the kernel is called again for a compute capability and set of argument types that is already in the definitions, the pre-compiled kernel is retrieved from the cache and called directly. If the compute capability or argument types are new and have not been seen before, the compiler pipeline is run again to compile a new specialisation.\n",
    "\n",
    "## Generating LLVM IR\n",
    "\n",
    "The next stage is lowering, which makes the LLVM IR.\n",
    "\n",
    "LLVM optimizes the IR. When we inspect the IR we see the optimized - unoptimized is very verbose (I would usually only look at it for debugging Numba's code generation).\n",
    "\n",
    "The process of translating the typed Numba IR into LLVM IR is relatively mechanical - there is a large number of functions registered inside Numba for \"lowering\" the Typed Numba IR into LLVM IR, which mostly proceed through translation one IR instruction at a time, generating the equivalent LLVM IR.\n",
    "\n",
    "The LLVM IR can be thought of as a machine-independent assembly language, and it has a relatively small instruction set of fairly low-level / primitive instructions (add, multiply, load, etc.). LLVM IR can be translated into many different target assembly languages - e.g. x86, ARM, PowerPC, PTX, etc - however, note that LLVM IR is not completely portable, and there are slight differences between LLVM IR for each machine. There are additional differences between the IR that is generated for PTX vs. other ISAs, in order to efficiently implement the mapping of work items to threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ((cc, types), llir) in axpy.inspect_llvm().items():\n",
    "    print(f'Compute capability {cc} / argtypes {types}:\\n')\n",
    "    print(llir)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLVM to PTX\n",
    "\n",
    "LLVM has a built-in JIT Compiler that is used for the CPU target. For PTX, we use NVVM because it has much better support for NVIDIA devices than the in-tree PTX backend, in terms of performance (and possibly feature support) but it does lag behind the LLVM release used by Numba (Numba uses LLVM 8, NVVM is somewhere between LLVM 3.4 and 5.0)\n",
    "\n",
    "We will pull in various internals of Numba to assemble a function that can be used to drive NVVM and get PTX out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba.core.compiler_lock import global_compiler_lock\n",
    "from numba.cuda.cudadrv import nvvm\n",
    "from numba.cuda.compiler import compile_cuda\n",
    "from numba import float32, int32, void\n",
    "\n",
    "\n",
    "with global_compiler_lock:\n",
    "    argtys = (float32[:], int32, float32[:], float32[:])\n",
    "    returnty = void\n",
    "    cres = compile_cuda(axpy.py_func, void, argtys, debug=False, inline=False)\n",
    "    fname = cres.fndesc.llvm_func_name\n",
    "    lib, kernel = cres.target_context.prepare_cuda_kernel(cres.library, fname,\n",
    "                                                          cres.signature.args,\n",
    "                                                          debug=False)\n",
    "    llvm_module = lib._final_module\n",
    "\n",
    "    cc = (5, 2)\n",
    "    arch = nvvm.get_arch_option(*cc)\n",
    "    llvmir = str(llvm_module)\n",
    "    ptx = nvvm.llvm_to_ptx(llvmir, opt=3, arch=arch)\n",
    "\n",
    "print(ptx.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're not interested in the way the internals are drive, we can also use `inspect_asm()` to get it for a function and argument types - here we just iterate over them all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ((cc, types), asm) in axpy.inspect_asm().items():\n",
    "    print(f'Compute capability {cc} / argtypes {types}:\\n')\n",
    "    print(asm)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a module\n",
    "\n",
    "Numba's compilation pipeline has finished with the generation of PTX. The next step towards execution is to use the driver API wrappers to create and load a module. All subsequent operations are completed with the driver API and Numba's wrappers around it.\n",
    "\n",
    "To create a module, we use a `Linker` object, that provides a Pythonic interface to the module management functions in the driver API. We add our PTX to the linker, and then inform the linker that we're finished to get back a cubin and some other information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba.cuda.cudadrv.driver import Linker\n",
    "\n",
    "linker = Linker()\n",
    "linker.add_ptx(ptx)\n",
    "cubin, size = linker.complete()\n",
    "\n",
    "compile_info = linker.info_log\n",
    "\n",
    "print(f'Module size: {size}')\n",
    "print(compile_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further reading: [The Module Management section](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MODULE.html) of the CUDA Driver API lists the functions that are involved in creating and linking a module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SASS\n",
    "\n",
    "The module we have just created contains SASS code - the assembly that runs on the GPU. This was JIT-compiled by the driver during the link.\n",
    "\n",
    "There's no easy way to dump the assembly from a cubin from within Python or the Driver API, so to inspect it, we are forced to write to to a file and run `cuobjdump` on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "\n",
    "cubinarr = np.ctypeslib.as_array(ctypes.cast(cubin,ctypes.POINTER(ctypes.c_char)), shape=(size,))\n",
    "\n",
    "with open('axpy.cubin', 'wb') as f:\n",
    "    f.write(cubinarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!file axpy.cubin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cuobjdump -sass axpy.cubin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no PTX in the module (note the empty output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cuobjdump -ptx axpy.cubin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the module\n",
    "\n",
    "The module has been generated, but the code needs to be loaded to the GPU via the driver API. Here we need to get a context to load it in.\n",
    "\n",
    "Numba mangles the names of Python functions when it JIT compiles them to distinguish between compiled implementations of the same function for different types, functions of the same name in different modules, etc. - this is a similar idea to C++ name mangling. The Itanium name-mangling ABI is followed by Numba, as it is commonly used on many platforms.\n",
    "\n",
    "Numba also prepends the namespace `cudapy` to names, so we must do the same to be able to retrieve a reference to the compiled function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba.cuda.cudadrv.driver import load_module_image\n",
    "from numba.core import itanium_mangler\n",
    "\n",
    "ctx = cuda.get_current_device().get_primary_context()\n",
    "module = load_module_image(ctx, cubin)\n",
    "\n",
    "mangled_name = itanium_mangler.prepend_namespace(fname, ns='cudapy')\n",
    "mangled_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get a reference to the compiled function, which will come back in a Python wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cufunc = module.get_function(mangled_name)\n",
    "\n",
    "type(cufunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reference can be used to launch a kernel on the GPU, but we must first prepare its arguments.\n",
    "\n",
    "## Preparing kernel arguments\n",
    "\n",
    "First we need to do a little bit of work to move data to the GPU, that Numba would usually do for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy our arrays to the device\n",
    "d_r = cuda.to_device(r)\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to prepare the list of arguments - passing a device array to a kernel is done by passing individual members of Numba's array object struct as parameters. These are:\n",
    "\n",
    "- `meminfo`: Not used on CUDA - on the CPU target it is used as part of Numba's reference counting implementation for managing allocations\n",
    "- `parent`: Not used in CUDA - on the CPU target, it is used for a pointer to the pyobject from which the array was created, in case object mode fallback is needed.\n",
    "- `nitems`: The number of items in the array (regardless of shape)\n",
    "- `itemsize`: The size of one item in the array\n",
    "- `data`: A pointer to the data\n",
    "- Shape and strides: The size of each dimension of the shape and passed first, then the stride of each dimension.\n",
    "\n",
    "This differs from how an array argument would usually be passed to a CUDA C kernel, where only a pointer to the data would be passed in, and the programmer would have a free choice about how to pass in any necessary information for iterating over the pointed-to data.\n",
    "\n",
    "### Creating array arguments\n",
    "\n",
    "We first need to define a helper function for generating the list of parameters for a given device array. This is based on code in `numba.cuda.compiler.CUDAKernel._prepare_args`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A couple of helpers from Numba's CUDA driver implementation\n",
    "from numba.cuda.cudadrv.driver import (device_pointer, is_device_memory,\n",
    "                                       device_ctypes_pointer)     # noqa\n",
    "from ctypes import addressof, c_void_p                            # noqa\n",
    "\n",
    "\n",
    "# Helper function to create all the args for an array structure\n",
    "def make_array_args(arr):\n",
    "    args = []\n",
    "    c_intp = ctypes.c_ssize_t\n",
    "\n",
    "    meminfo = ctypes.c_void_p(0)\n",
    "    parent = ctypes.c_void_p(0)\n",
    "    nitems = c_intp(arr.size)\n",
    "    itemsize = c_intp(arr.dtype.itemsize)\n",
    "    data = ctypes.c_void_p(device_pointer(arr))\n",
    "\n",
    "    args.append(meminfo)\n",
    "    args.append(parent)\n",
    "    args.append(nitems)\n",
    "    args.append(itemsize)\n",
    "    args.append(data)\n",
    "\n",
    "    for ax in range(arr.ndim):\n",
    "        args.append(c_intp(arr.shape[ax]))\n",
    "    for ax in range(arr.ndim):\n",
    "        args.append(c_intp(arr.strides[ax]))\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the kernel argument list\n",
    "\n",
    "Next we create the list of arguments using the helper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of arguments - we compiled for float32[:], int32, float32[:],\n",
    "# float32[:], so our second argument will be of int type.\n",
    "args = []\n",
    "args += make_array_args(d_r)\n",
    "args += [ctypes.c_int(13)]  \n",
    "args += make_array_args(d_x)\n",
    "args += make_array_args(d_y)\n",
    "\n",
    "# Make a list of pointers to the arguments\n",
    "param_vals = []\n",
    "for arg in args:\n",
    "    if is_device_memory(arg):\n",
    "        param_vals.append(addressof(device_ctypes_pointer(arg)))\n",
    "    else:\n",
    "        param_vals.append(addressof(arg))\n",
    "\n",
    "params = (c_void_p * len(param_vals))(*param_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launching the kernel\n",
    "\n",
    "At this point we have everything ready to call `cuLaunchKernel` from the driver API. We will make use of the `driver` instance from `numba.cuda.cudadrv.driver`, as it handles the use of this function with `ctypes`, as well as wrapping it in a function that converts non-zero return codes into Python exceptions (which saves us having to check the return code).\n",
    "\n",
    "This code is inspired by that in `numba.cuda.cudadrv.driver.launch_kernel` (usually called from `Function.__call__`, which is called by `numba.cuda.compiler.CUDAKernel._kernel_call`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (see cudadrv.launch_kernel)\n",
    "\n",
    "# CUresult cuLaunchKernel(CUfunction f,\n",
    "#                        unsigned int gridDimX,\n",
    "#                        unsigned int gridDimY,\n",
    "#                        unsigned int gridDimZ,\n",
    "#                        unsigned int blockDimX,\n",
    "#                        unsigned int blockDimY,\n",
    "#                        unsigned int blockDimZ,\n",
    "#                        unsigned int sharedMemBytes,\n",
    "#                        CUstream hStream, void **kernelParams,\n",
    "#                        void ** extra)\n",
    "\n",
    "from numba.cuda.cudadrv.driver import driver\n",
    "\n",
    "driver.cuLaunchKernel(cufunc.handle,\n",
    "                      blocks, 1, 1,\n",
    "                      threads, 1, 1,\n",
    "                      0,\n",
    "                      0,\n",
    "                      params,\n",
    "                      None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we should try copying the result `d_r` back to the host and check it is as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_r = d_r.copy_to_host()\n",
    "reference = x * 13 + y \n",
    "print(h_r)\n",
    "print(reference)\n",
    "\n",
    "# Sanity check\n",
    "np.testing.assert_allclose(reference, h_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all has gone well, we should have two similar-looking arrays, and no assertion firing.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "Wasn't that fun?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
